{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb39d7a",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "\n",
    "Дополнительная часть (необязательная)\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1255e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_url = 'https://habr.com'                                                           # голова адреса страницы\n",
    "url = head_url + '/ru/all/'                                                             # адрес страницы\n",
    "keywords = ['python', 'парсинг']                                                        # список слов для поиска\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70'\n",
    "          }\n",
    "keywords_search = '|'.join(keywords).lower()                                            # строка поиска в формате RegExp\n",
    "params = {}                                                                             # словарь параметров\n",
    "df_result = pd.DataFrame()                                                              # DataFrame результатов поиска\n",
    "\n",
    "def find_item(head_url, post):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70'}\n",
    "    header = post.find('a', class_='tm-article-snippet__title-link')                    # делим пост на заголовок\n",
    "    post_header = header.text\n",
    "    post_review = post.find('div', class_='article-formatted-body').text                #                        и обзор\n",
    "    post_link   = head_url + header.attrs.get('href')                                   # из заголовка получаем ссылку  \n",
    "    req = requests.get(post_link, headers=headers)                                      # по ссылке запрашиваем содержимое\n",
    "    if req.ok:                                                                          # если содержимое получено\n",
    "        soup =  BeautifulSoup(req.text, 'html.parser')                                  \n",
    "        post_body = soup.find('div', class_='tm-article-body').text                     # то достаем из него текст статьи\n",
    "    else:\n",
    "        post_body =''\n",
    "    if (re.search(fr\"{keywords_search}\",                                                # если находим ключевые слова\n",
    "                  post_header                                                           # в заголовке\n",
    "                  + post_review                                                         # обзоре\n",
    "                  + post_body)):                                                        # или самом тексте статьи,\n",
    "        post_date = (post.find('span', class_='tm-article-snippet__datetime-published') # то получаем дату из поста    \n",
    "                     .time                                                              \n",
    "                     .attrs                                                             \n",
    "                     .get('datetime'))                                                                    \n",
    "        return {'Дата': post_date,                                                      # и возвращаем результат в виде словаря\n",
    "                'Заголовок': str(post_header).strip(), \n",
    "                'Ссылка': post_link, \n",
    "                'Текст статьи': str(post_body).strip()}\n",
    "    else:                                                                               # если совпадений не найдено,\n",
    "        return False                                                                    # то возвращаем False\n",
    "    \n",
    "req = requests.get(url, headers=headers)                                                # читаем содержимое с заданной страницы\n",
    "soup = BeautifulSoup(req.text, 'html.parser') \n",
    "page_count = int(soup.find_all('a', class_=\"tm-pagination__page\")[-1].text.strip())     # ищем число страниц\n",
    "for page in range(1, page_count + 1):                                                   # проходим по каждой странице \n",
    "    params['page'] = page                                                               # в параметры передаем номер текущей страницы\n",
    "    req = requests.get(url, params, headers=headers)                                    # запрашиваем содержимое текущей страницы\n",
    "    if req.ok:                                                                          # если результат запроса удачный\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')                                   # читаем текст в суп\n",
    "        posts = soup.find_all('div', class_='tm-article-snippet')                       # разбиваем на посты\n",
    "        for post in posts:                                                              # проходим по каждому посту\n",
    "            item = find_item(head_url, post)                                            # передаем шапку адреса и пост, получаем словарь или False\n",
    "            time.sleep(0.3)\n",
    "            if item:                                                                    # если вернулся словарь, то добавляем в DF\n",
    "                df_result = df_result.append(item, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10129",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "Обязательная часть\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\n",
    "\n",
    "Дополнительная часть (необязательная)\n",
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.get\n",
    "\n",
    "GROUP = 'netology'  \n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c24d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2a3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [400]>\n"
     ]
    }
   ],
   "source": [
    "url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 YaBrowser/21.6.1.274 Yowser/2.5 Safari/537.36',\n",
    "\n",
    "           'Vaar-Version': '0'\n",
    "Accept: application/json, text/plain, */*\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Accept-Language: ru,en;q=0.9\n",
    "Connection: keep-alive\n",
    "Content-Length: 31\n",
    "Content-Type: application/json;charset=UTF-8\n",
    "Host: identityprotection.avast.com\n",
    "Origin: https://www.avast.com\n",
    "Referer: https://www.avast.com/\n",
    "sec-ch-ua: \" Not;A Brand\";v=\"99\", \"Yandex\";v=\"91\", \"Chromium\";v=\"91\"\n",
    "sec-ch-ua-mobile: ?0\n",
    "Sec-Fetch-Dest: empty\n",
    "Sec-Fetch-Mode: cors\n",
    "Sec-Fetch-Site: same-site\n",
    "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 YaBrowser/21.6.1.274 Yowser/2.5 Safari/537.36\n",
    "Vaar-Header-App-Build-Version: 1.0.0\n",
    "Vaar-Header-App-Product: hackcheck-web-avast\n",
    "Vaar-Header-App-Product-Name: hackcheck-web-avast\n",
    "Vaar-Version: 0           \n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "           \n",
    "          }\n",
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']\n",
    "        \n",
    "jsn ={'emailAddresses': EMAIL}\n",
    "res = requests.post(url, json=jsn, headers=headers)\n",
    "if res.status_code == '200':\n",
    "    df_breaches = pd.DataFrame(res.json()['breaches']).T.loc[:, ['publishDate', 'site', 'description']]\n",
    "else: \n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_breaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184533d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
