{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb39d7a",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм вида: <дата> - <заголовок> - <ссылка>\n",
    "\n",
    "Дополнительная часть (необязательная)\n",
    "Улучшить скрипт так, чтобы он анализировал не только preview-информацию статьи, но и весь текст статьи целиком.\n",
    "\n",
    "Для этого потребуется получать страницы статей и искать по тексту внутри этой страницы.\n",
    "\n",
    "Итоговый датафрейм формировать со столбцами: <дата> - <заголовок> - <ссылка> - <текст_статьи>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1255e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_url = 'https://habr.com'                                                           # голова адреса страницы\n",
    "url = head_url + '/ru/all/'                                                             # адрес страницы\n",
    "keywords = ['python', 'парсинг']                                                        # список слов для поиска\n",
    "headers = {'User-Agent': \n",
    "           'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70'\n",
    "          }\n",
    "keywords_search = '|'.join(keywords).lower()                                            # строка поиска в формате RegExp\n",
    "params = {}                                                                             # словарь параметров\n",
    "df_result = pd.DataFrame()                                                              # DataFrame результатов поиска\n",
    "\n",
    "def find_item(head_url, post):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70'}\n",
    "    header = post.find('a', class_='tm-article-snippet__title-link')                    # делим пост на заголовок\n",
    "    post_header = header.text\n",
    "    post_review = post.find('div', class_='article-formatted-body').text                #                        и обзор\n",
    "    post_link   = head_url + header.attrs.get('href')                                   # из заголовка получаем ссылку  \n",
    "    req = requests.get(post_link, headers=headers)                                      # по ссылке запрашиваем содержимое\n",
    "    if req.ok:                                                                          # если содержимое получено\n",
    "        soup =  BeautifulSoup(req.text, 'html.parser')                                  \n",
    "        post_body = soup.find('div', class_='tm-article-body').text                     # то достаем из него текст статьи\n",
    "    else:\n",
    "        post_body =''\n",
    "    if (re.search(fr\"{keywords_search}\",                                                # если находим ключевые слова\n",
    "                  post_header                                                           # в заголовке\n",
    "                  + post_review                                                         # обзоре\n",
    "                  + post_body)):                                                        # или самом тексте статьи,\n",
    "        post_date = (post.find('span', class_='tm-article-snippet__datetime-published') # то получаем дату из поста    \n",
    "                     .time                                                              \n",
    "                     .attrs                                                             \n",
    "                     .get('datetime'))                                                                    \n",
    "        return {'Дата': post_date,                                                      # и возвращаем результат в виде словаря\n",
    "                'Заголовок': str(post_header).strip(), \n",
    "                'Ссылка': post_link, \n",
    "                'Текст статьи': str(post_body).strip()}\n",
    "    else:                                                                               # если совпадений не найдено,\n",
    "        return False                                                                    # то возвращаем False\n",
    "    \n",
    "req = requests.get(url, headers=headers)                                                # читаем содержимое с заданной страницы\n",
    "soup = BeautifulSoup(req.text, 'html.parser') \n",
    "page_count = int(soup.find_all('a', class_=\"tm-pagination__page\")[-1].text.strip())     # ищем число страниц\n",
    "for page in range(1, page_count + 1):                                                   # проходим по каждой странице \n",
    "    params['page'] = page                                                               # в параметры передаем номер текущей страницы\n",
    "    req = requests.get(url, params, headers=headers)                                    # запрашиваем содержимое текущей страницы\n",
    "    if req.ok:                                                                          # если результат запроса удачный\n",
    "        soup = BeautifulSoup(req.text, 'html.parser')                                   # читаем текст в суп\n",
    "        posts = soup.find_all('div', class_='tm-article-snippet')                       # разбиваем на посты\n",
    "        for post in posts:                                                              # проходим по каждому посту\n",
    "            item = find_item(head_url, post)                                            # передаем шапку адреса и пост, получаем словарь или False\n",
    "            time.sleep(0.3)\n",
    "            if item:                                                                    # если вернулся словарь, то добавляем в DF\n",
    "                df_result = df_result.append(item, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc10129",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "Обязательная часть\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата утечки> - <источник утечки> - <описание утечки>\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы.\n",
    "\n",
    "Дополнительная часть (необязательная)\n",
    "Написать скрипт, который будет получать 50 последних постов указанной группы во Вконтакте.\n",
    "Документация к API VK: https://vk.com/dev/methods , вам поможет метод wall.get\n",
    "\n",
    "GROUP = 'netology'  \n",
    "TOKEN = УДАЛЯЙТЕ В ВЕРСИИ ДЛЯ ПРОВЕРКИ, НА GITHUB НЕ ВЫКЛАДЫВАТЬ\n",
    "В итоге должен формироваться датафрейм со столбцами: <дата поста> - <текст поста>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6c24d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d2a3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [404]>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-5c37b5cc5ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"emailAddresses\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"xxx@x.ru\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, headers=headers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdict_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\cher_\\myds\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    898\u001b[0m                     \u001b[1;31m# used.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m                     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 900\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "url0 = 'https://www.avast.com/hackcheck'\n",
    "url = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = { 'Accept': 'application/json, text/plain, */*',\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Accept-Language': 'ru,en;q=0.9,en-GB;q=0.8,en-US;q=0.7',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Content-Length': '31',\n",
    "            'Content-Type': 'application/json;charset=UTF-8',\n",
    "            'Host': 'identityprotection.avast.com',\n",
    "            'Origin': 'https://www.avast.com',\n",
    "            'Referer': 'https://www.avast.com/',\n",
    "            'sec-ch-ua': '\" Not;A Brand\";v=\"99\", \"Microsoft Edge\";v=\"91\", \"Chromium\";v=\"91\"',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'Sec-Fetch-Dest': 'empty',\n",
    "            'Sec-Fetch-Mode': 'cors',\n",
    "            'Sec-Fetch-Site': 'same-site',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.70'\n",
    "           ,'Vaar-Header-App-Build-Version': '1.0.0',\n",
    "            'Vaar-Header-App-Product': 'hackcheck-web-avast',\n",
    "            'Vaar-Header-App-Product-Name': 'hackcheck-web-avast',\n",
    "            'Vaar-Version': '0'\n",
    "          }\n",
    "#jsn = {\"emailAddresses\":[\"xxx@x.ru\"]}\n",
    "resp = requests.get(url, json={\"emailAddresses\":[\"xxx@x.ru\"]})#, headers=headers)\n",
    "print(resp)\n",
    "dict_res = dict(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "684d7db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(dict_res['breaches'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3a03df30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishDate</th>\n",
       "      <th>site</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016-10-29T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>Popular Russian social networking platform VKo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2016-10-23T00:00:00Z</td>\n",
       "      <td>imesh.com</td>\n",
       "      <td>In June 2016, a cache of over 51 million user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>2017-01-31T00:00:00Z</td>\n",
       "      <td>cdprojektred.com</td>\n",
       "      <td>In March 2016, CDProjektRed.com.com's forum da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>cfire.mail.ru</td>\n",
       "      <td>In July and August of 2016, two criminals carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>parapa.mail.ru</td>\n",
       "      <td>In July and August 2016, two criminals execute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               publishDate              site  \\\n",
       "3     2016-10-21T00:00:00Z         adobe.com   \n",
       "12    2016-10-29T00:00:00Z            vk.com   \n",
       "15    2016-10-23T00:00:00Z         imesh.com   \n",
       "2961  2017-01-31T00:00:00Z  cdprojektred.com   \n",
       "3164  2017-02-14T00:00:00Z     cfire.mail.ru   \n",
       "3176  2017-02-14T00:00:00Z    parapa.mail.ru   \n",
       "\n",
       "                                            description  \n",
       "3     In October of 2013, criminals penetrated Adobe...  \n",
       "12    Popular Russian social networking platform VKo...  \n",
       "15    In June 2016, a cache of over 51 million user ...  \n",
       "2961  In March 2016, CDProjektRed.com.com's forum da...  \n",
       "3164  In July and August of 2016, two criminals carr...  \n",
       "3176  In July and August 2016, two criminals execute...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_res.T.loc[:, ['publishDate', 'site', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ee99181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-10-21T00:00:00Z\n",
      "adobe.com\n",
      "In October of 2013, criminals penetrated Adobe's corporate network and the stole source code for several of its software products. The affected products included Adobe's ColdFusion web application platform as well as the Acrobat suite of products. Adobe asserts that criminals also accessed nearly three-million customer credit card records and stole login data for an undisclosed number of Adobe user accounts.\n",
      "------\n",
      "2016-10-29T00:00:00Z\n",
      "vk.com\n",
      "Popular Russian social networking platform VKontakte was breached in late 2012. Over 100 million clear-text passwords were compromised in the breach. Breached credential sets included victims' e-mail addresses, passwords, dates of birth, phone numbers and location details. The credential set was advertised on a dark web marketplace as of June 2016 for a price of one bitcoin. \n",
      "------\n",
      "2016-10-23T00:00:00Z\n",
      "imesh.com\n",
      "In June 2016, a cache of over 51 million user credentials from the online sharing service iMesh appeared for sale on a dark web marketplace.  The database contained user email addresses, usernames, passwords, IP addresses and location.\n",
      "------\n",
      "2017-01-31T00:00:00Z\n",
      "cdprojektred.com\n",
      "In March 2016, CDProjektRed.com.com's forum database was breached. The attacker may have exploited a vulnerability in vBulletin. The stolen data contains over 1,870,000 user records including associated usernames, email addresses, hashed passwords, and salts. The leaked credentials are being shared and sold privately on the dark web.\n",
      "------\n",
      "2017-02-14T00:00:00Z\n",
      "cfire.mail.ru\n",
      "In July and August of 2016, two criminals carried out attacks on three separate forums hosted by Mail.ru, including CFire. The hackers used known SQL injection vulnerabilities found in older vBulletin forum software to obtain access to the databases. Shortly after the breach itself, the contents of CFire's database were leaked publicly. The database contains usernames, email addresses, and MD5 hashed passwords for just under 13 million users.\n",
      "------\n",
      "2017-02-14T00:00:00Z\n",
      "parapa.mail.ru\n",
      "In July and August 2016, two criminals executed attacks against three separate forums hosted by Mail.ru including the Russian forum Parapa. Shortly after the breach occurred, the contents of Parapa's database were leaked publicly. The database contains usernames, email addresses, and hashed passwords for around 5 million users.\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for item in dict_res['breaches'].values():\n",
    "    print(item['publishDate'])\n",
    "    print(item['site'])\n",
    "    print(item['description'])\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e53afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
